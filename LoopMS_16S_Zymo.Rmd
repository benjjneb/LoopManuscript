---
title: "Evaluationg LoopSeq 16S on Zymo Mock"
author: "BJC"
date: "2021-02-02"
output: html_document
---

## Setup

Load libraries, set the working directory to the location of this Rmarkdown file (only necessary when running by hand), and read in filenames:
```{r, echo=FALSE}
library(dada2, quietly=TRUE); packageVersion("dada2")
library(ShortRead, quietly=TRUE)
library(ggplot2, quietly=TRUE)
library(reshape2, quietly=TRUE)
set.seed(100)
setwd("~/LoopManuscript") # CHANGE ME to the location of this file
path <- "~/LoopData/16S/Zymo" # CHANGE ME to the location of the fastq files
path.fig <- "Figures" # Relative path to where figures should be saved
path.rds <- "RDS" # Relative path to where RDS should be saved
fn <- file.path(path, "Zymo_contig_list_trimmed.fq")
length(getSequences(fn))
```

# Filtering and Trimming

Check the complexity profile of the raw data:
```{r}
plotComplexity(fn)
```

No evidence of any low complexity sequence issues.

Inspect the quality profile of the raw data:
```{r, warning=FALSE, message=FALSE}
plotQualityProfile(fn)
```

Generally very high quality. Quality a bit lower in the first ~50 nts. The big quality dropoff at >1500 nts is driven by a small number of reads with extended lengths, and is beyond the length of the sequenced amplicon anyway. That should be almost entirely removed by a length filter.

Enter primers, and confirm their presence and the overall orientation of the reads. Code is adapated from [the DADA2 ITS tutorial workflow](https://benjjneb.github.io/dada2/ITS_workflow.html#identify-primers):
```{r}
FWD <- "AGAGTTTGATCMTGGC" # Loop 16S forward primer
REV <- "TACCTTGTTACGACTT" # Loop 16S reverse primer
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.Primer = sapply(allOrients(FWD), primerHits, fn = fn), 
      REV.Primer = sapply(allOrients(REV), primerHits, fn = fn))
```

The FWD and REV primers are found in their expected orientations in the vast majority of the reads (~19k/21k) as expected.

Remove the primers and any flanking sequence from the reads, and filter out reads that don't contain both primers:
```{r}
nop <- file.path(path, "nop", basename(fn))
out <- removePrimers(fn, nop, FWD, rc(REV), verbose=TRUE)
```

Almost none are being reverse-complemented. The assembly step in the Loop contig construction workflow already orients the reads in a consistent direction.

Review the quality profile after primer removal:
```{r, message=FALSE}
plotQualityProfile(nop)
```

The low-quality long-read tail was removed, but it still appears worthwhile to enforce a length window at the trimming stage to get rid of the last few too-long sequences.

Filter the sequences and enforce minimum/maximum lengths appropriate for full-length 16S:
```{r}
filt <- file.path(path, "filtered", basename(fn))
track <- filterAndTrim(nop, filt, maxEE=2, minLen=1400, maxLen=1600, verbose=TRUE)
```

Final inspection of the quality profile:
```{r, message=FALSE}
plotQualityProfile(filt)
```

Very clean. Lower quality at the start and ends of the reads, as is expected from Loop sequencing as there will be less coverage at the ends of the contigs, but high-quality throughout.

# Scan for contaminants in filtered reads

We'll do a simple scan for potential contaminants in the filtered unique sequences by assigning them taxonomy, and attributing any sequences assigned to genera outside those expected as potential contaminants:
```{r}
drp.filt <- derepFastq(filt)
tax.filt.rds <- file.path(path.rds, "tax_filt.rds") # RDS save/load to speed up reruns of the code
if(!file.exists(tax.filt.rds)) {
  tax.filt <- assignTaxonomy(drp.filt, "~/tax/silva_nr99_v138_train_set.fa.gz", minBoot=80, multi=TRUE)
  saveRDS(tax.filt, tax.filt.rds)
}
tax.filt <- readRDS(tax.filt.rds)
if(!identical(getSequences(tax.filt), getSequences(drp.filt))) stop("Taxonomy mismatch.")
table(tax.filt[,"Genus"], useNA="ifany")
```

Everything is in the expected genera, except for a single `NA`. Taking a closer look at that with BLAST against nt:
```{r}
dada2:::pfasta(getSequences(drp.filt)[is.na(tax.filt[,"Genus"])])
```

Not a contaminant, looks like a chimera of some sort. We conclude here that there are no contaminants present in the filtered data.

# Denoising

Learn the error rates:
```{r, warning=FALSE}
err <- learnErrors(filt, multi=TRUE, verbose=0)
plotErrors(err, nominalQ=TRUE)
```

It would be preferable to have more data than this, especially given the seemingly very low error rates, but this is decent matching between the error model and the data. Possible improvements to the DADA2 loess error model fitting could be made in how it is working at the highQ tail, where the slightly degraded fitting is probably driven by the interaction between weighted fitting and the very high fraction of maximum quality scores.

Denoise the filtered data into ASVs, using current DADA2 defaults, except for `OMEGA_C=0` to correct all reads (for the purpose of identifying all errors later one):
```{r}
dd <- dada(filt, err, multi=TRUE, OMEGA_C=0)
dd
```

Assign taxonomy:
```{r}
tax <- tax.filt[getSequences(dd),]
if(!identical(getSequences(tax), getSequences(dd))) stop("Taxonomy mismatch.")
table(tax[,"Genus"], useNA="ifany")
```

All are from the expected genera of the Zymo mock community.

# Accuracy of DADA2 ASVs on Zymo mock community

*Note: The code and approach in this section is slightly modified from that used in Callahan et al, NAR, 2019 on PacBio full-length 16S sequencing.*

Print out all reads and BLAST them against nt and the (incomplete) set of SSU reference sequences provided by Zymo for these strains:
```{r}
asvs <- getSequences(dd)
names(asvs) <- paste(tax[,"Genus"], seq(nrow(tax)))
dada2:::pfasta(asvs, id=names(asvs))
## 
## dada2:::pfasta(asvs, id=paste(tax[,"Genus"], seq(nrow(tax))))
## BLAST fasta against nt excluding uncultured/environmental sequences: https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&BLAST_SPEC=&LINK_LOC=blasttab&LAST_PAGE=blastn
fn.zymo.ssu <- ""
sapply(asvs, )
```

Results of BLAST search on Feb 2, 2020 are recorded.

 1. Exact match (100% identity, 100% coverage) to S. aureus
 2. Exact match to S. enterica
 3. Exact match to E. faecalis
 4. Exact match to L. monocytogenes
 5. Exact match to B. subtilis
 6. Exact match to L. monocytogenes
 7. Exact match to P. aeruginosa
 8. Exact match to B. subtilis
 9. Exact match to E. coli
10. Exact match to L. monocytogenes
11. Exact match to L. fermentum
12. Exact match to S. aureus
13. Exact match to L. fermentum
14. Exact match to S. aureus
15. Exact match to B. subtilis
16. Exact match to L. fermentum
17. Exact match to B. subtilis
18. Exact match to B. subtilis
19. One mismatch to best hit (L. fermentum)
20. Exact match to L. fermentum
21. Exact match to E. coli
22. Exact match to E. coli
23. Exact match to B. subtilis
24. Exact match to E. coli
25. Exact match to E. coli
26. One mismatch to best hit (S. enterica)
27. Exact match to E. coli

Store these results in R format:
```{r}
exact.nt <- c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,
              TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE,TRUE,
              TRUE, TRUE, TRUE, TRUE, TRUE, FALSE,TRUE)
```

In addition, all ASVs were were also matched exactly against the (incomplete) 16S rRNA gene sequences references provided by Zymo research with this microbial standard. The reference file queried here is derived from https://s3.amazonaws.com/zymo-files/BioPool/ZymoBIOMICS.STD.refseq.v2.zip by concatenating together the 16S fastas for each of the eight bacteria.

```{r}
fnz <- "Docs/Zymo_Bacteria_16S_v2.fasta"
refz <- getSequences(fnz)
exact.zymo <- sapply(asvs, function(asv) any(grepl(asv,refz)) )
which(!exact.nt & !exact.zymo)
```

Every denoised ASV except for one (ASV19: L. fermentum) is an exact (100\% identity and 100\% coverage) match to a previously sequenced 16S rRNA gene from the expected species (re-confirmed Feb 2, 2021). ASV19 has a single-nucleotide mismatch to the closest previously sequenced L. fermentum 16S rRNA gene available in the union of the NCBI nt database and the provided Zymo SSU rRNA reference sequences.

Create barplot of ASVs genome-scaled abundances by genus. 
```{r}
theme_set(theme_bw())
genusPalette <- c(Bacillus="#e41a1c", Enterococcus="#377eb8", Escherichia="#4daf4a", Lactobacillus="#984ea3",
                  Listeria="#ff7f00", Pseudomonas="#ffff33", Salmonella="#a65628", Staphylococcus="#f781bf")
ncopy <- c("Pseudomonas"=4, "Escherichia"=7, "Salmonella"=7, "Lactobacillus"=5, 
           "Enterococcus"=4, "Staphylococcus"=6, "Listeria"=6, "Bacillus"=10)

st <- makeSequenceTable(dd)
tt <- tax
tt[tt[,6] ==  "Escherichia/Shigella",6] <- "Escherichia"

abund.ome <- sapply(names(ncopy), function(gen) {
  is.gen <- grepl(gen, tax[,"Genus"])
  sum(dd$denoised[is.gen])/ncopy[gen]
})
names(abund.ome) <- names(ncopy)
dfgen <- data.frame(Genus=names(ncopy), Abundance=abund.ome, stringsAsFactors = FALSE)
ggplot(data=dfgen, aes(x=Genus, y=Abundance)) + 
  geom_col(width=0.4, aes(fill=Genus)) + scale_fill_manual(values=genusPalette) +
  ylim(c(0, NA)) + geom_hline(yintercept=mean(abund.ome), linetype="dashed") +
  theme(axis.text.x=element_blank(), axis.ticks.x = element_blank()) +
  ylab("Genome Abundance")
# Similar profile to PB data, although again remember this is a different batch
dfasv <- data.frame(Genus=tt[,"Genus"], Abundance=st[1,], stringsAsFactors = FALSE)
rownames(dfasv) <- NULL
ggplot(data=dfasv, aes(x=Genus, y=Abundance)) + 
  geom_point(aes(color=Genus), shape="x", size=4) + scale_color_manual(values=genusPalette) +
  ylim(c(0, NA)) +
  theme(axis.text.x=element_blank(), axis.ticks.x = element_blank()) +
  ylab("ASV Abundance")
# Looks similar, with some differences related to the different strains in this batch versus the original batch
dfasv$ScaledAbundance <- dfasv$Abundance/abund.ome[dfasv$Genus]
# Number the ASVs in each strain/genus
dfasv$Variant <- sapply(seq(nrow(dfasv)), function(i) sum(dfasv$Genus[1:i] == dfasv$Genus[[i]], na.rm=TRUE))
p.stoich <- ggplot(data=dfasv, aes(x=Variant, y=ScaledAbundance, fill=Genus, width=0.5)) + geom_col() + 
  scale_fill_manual(values=genusPalette) +
  facet_wrap(~Genus, nrow=2) +
  scale_y_continuous(breaks=seq(0,round(max(dfasv$ScaledAbundance))), minor_breaks=NULL) +
  theme(panel.grid.major.y=element_line(color="grey60", size=0.2)) +
  theme(panel.grid.major.x=element_blank(), panel.grid.minor.x=element_blank()) +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  xlab("Full-length 16S Sequence Variants") + 
  ylab("Abundance (per-genome)") + 
  guides(fill=FALSE)
p.stoich
ggsave(file.path(path.fig, "ZymoASVs_Loop_DADA2Defaults.pdf"), p.stoich, width=8, height=5, units="in", useDingbats=FALSE)
```

Just as we saw in the PacBio profiling of the Zymo mock community (NOTE: The PacBio profilling was on a different older batch of this mock community, and several strains were switched between that batch and the newer batch profiled here), the different 16S alleles occur in the integral values consistent with intra-genomic allelic variation between the expected number of 16S copies in each of these genomes. We conclude from the joint evidence based on exact matching to previously sequenced genes from the same species, and from the integral ratios between alleles (including the single allele of *L. fermentum* that wasn't an exact match to a previously sequenced gene) that there are no residual errors in the denoised data.

How many reads are completely error free? Consider reads before and after quality filtering and length screening, but note that both sets of reads are the subset of the raw reads that passed through the primer detection removal step.
```{r}
drp.nop <- derepFastq(nop)
drp.filt <- derepFastq(filt)
tableCorrect <- function(query.unqs, ref.seqs) {
  is.correct <- getSequences(query.unqs) %in% getSequences(ref.seqs)
  c(Correct=sum(getUniques(query.unqs)[is.correct]),
    Incorrect=sum(getUniques(query.unqs)[!is.correct]))
}
cat("Primers detected and removed, but no filtering.\n")
tab.nop <- tableCorrect(drp.nop, dd)
tab.nop
tab.nop/sum(tab.nop)
cat("\nFiltered and length-selected.\n")
tab.filt <- tableCorrect(drp.filt, dd)
tab.filt
tab.filt/sum(tab.filt)
cat("\nReads removed during filtering and length-selection.\n")
tab.filtered_out <- tableCorrect(drp.nop, dd) - tableCorrect(drp.filt, dd)
tab.filtered_out
tab.filtered_out/sum(tab.filtered_out)
```

That is pretty remarkable, 94.6% of all reads were without any errors! (after filtering). Furthermore, filtering and length selecting were highly accurate. 98% of the reads removed by filtering/length removal had errors, which is remarkable given the starting pool was highly enriched in correct sequences. Some accuracy stats:

```{r}
# Note "Incorrect" are the "correct" true positives here, as they are errors that were caught by the filter
filt.sens <- tab.filtered_out["Incorrect"]/tab.nop["Incorrect"]
filt.spec <- tab.filtered_out["Correct"]/tab.nop["Correct"]
filt.prec <- tab.filtered_out["Incorrect"]/sum(tab.filtered_out)
filt.F1 <- 2*filt.prec*filt.sens/(filt.prec+filt.sens)
cat("Filtering (Defaults) Accuracy Stats\n", 
    "\tSensitivity:", filt.sens, "\n\tSpecificity:", filt.spec,
    "\n\tPrecision:", filt.prec, "\n\tF1 score:", filt.F1, "\n")
```

Highly specific filter. Return to this, as it could probably be even better. This is using the default reocmmendation of `maxEE=2`. What is the best threshold for Loop data?

# Accuracy of (default) filtered reads on Zymo mock community, by error type, position and quality score

Define functions to identify and collate errors by type, position and quality score:
```{r}
library(Biostrings)
get.diffs <- function(query, ref, vec=TRUE, ...) {
  al <- nwalign(query, ref, vec=vec, ...)
  cstr <- compareStrings(al[[1]], al[[2]])
  cstr.ref <- gsub("[+]", "", cstr) # Ref coordinates, but lose insertion info
  cstr.q <- gsub("[-]", "", cstr) # Query coordinates, but lose deletion info
  cstr.ins <- gsub("[+]+", "+", cstr) # Reduce all multi-base inserts to a single insert
  cstr.del <- gsub("[-]+", "-", cstr) # Reduce all multi-base deletions to a single deletion
  refpos.sub <- unlist(gregexpr(pattern='[?]', cstr.ref))
  refpos.del <- unlist(gregexpr(pattern='[-]+', cstr.ref)) # Just getting loc of first deleted base of multi-nt deletions
  refpos.ins <- unlist(gregexpr(pattern='[+]', cstr.ins))
  refpos.ins <- refpos.ins - seq_along(refpos.ins) + 1 # Correct back to ref coords
  qpos.sub <- unlist(gregexpr(pattern='[?]', cstr.q))
  qpos.ins <- unlist(gregexpr(pattern='[+]+', cstr.q)) # Just getting loc of first inserted base of multi-nt inserts
  qpos.del <- unlist(gregexpr(pattern='[-]', cstr.del))
  qpos.del <- qpos.del - seq_along(qpos.del) + 1 # Correct back to ref coords
  rv <- rbind( data.frame(Type="S", RefPos=refpos.sub, QueryPos=qpos.sub),
               data.frame(Type="D", RefPos=refpos.del, QueryPos=qpos.del),
               data.frame(Type="I", RefPos=refpos.ins, QueryPos=qpos.ins))
  rv[rv$RefPos > -1,]
}

df.diffs <- function(i, drp, dd) {
  df <- get.diffs(getSequences(drp)[i], dd$sequence[dd$map[i]])
  df <- cbind(df, Abund = rep(drp$uniques[i], nrow(df)), 
              Derep=rep(i, nrow(df)), Denoised=rep(dd$map[i], nrow(df)))
  df$Qual <- drp$quals[cbind(df$Derep, df$QueryPos)]
  df$Qual[df$Type == "D"] <- NA # Deletions don't have associated quality scores
  df
}
# Test
rf <- "AAAAAAAAGCATGCATGCATGCATGCAT" # Sub at 4, Del at 13, Ins at 29 (refpos)
qq <- "AAACAAAAGCATCATGCATGCATGCATA"
get.diffs(qq, rf)
```

Identify the subset of filtered reads to keep in this analysis, i.e. exclude contaminants and chimeras. Note however that there were no contaminants identified in this data previously, so just need to identify and remove chimeric filtered reads:
```{r}
bim.filt <- isBimeraDenovo(drp.filt, minFoldParentOverAbundance=3.5, multi=TRUE)
table(bim.filt)
table(drp.filt$uniques[bim.filt])
```

#Q to self: Should I keep the chimeras in for this?*

Collate all errors from non-contaminant/non-chimeric reads:
```{r}
ii.keep <- which(!bim.filt) ###Q
diffs.keep <- lapply(ii.keep, df.diffs, drp=drp.filt, dd=dd) # ~2 mins
diffs.keep <- do.call(rbind, diffs.keep)
diffs.keep <- diffs.keep[order(diffs.keep$Qual, decreasing=TRUE),]
rownames(diffs.keep) <- NULL
###
nnt.keep <- sapply(seq(max(nchar(getSequences(drp.filt)[ii.keep]))), function(pos) {
  sum(drp.filt$uniques[!bim.filt & nchar(getSequences(drp.filt)) >= pos])
})
table(is.na(diffs.keep$Qual), diffs.keep$Type)
```

Double-check with table of errors per read, make sure it matches the correct/incorrect sequence numbers.

```{r}
collated.incorrect.unqs <- unique(c(diffs.keep$Derep, which(bim.filt)))
collated.incorrect.reads <- sum(drp.filt$uniques[collated.incorrect.unqs])
if(!collated.incorrect.reads == tab.filt["Incorrect"]) {
  stop("Unexpected numbers of error-containing reads after collation")
}
table(table(diffs.keep$Derep))
```

Vast majority have just one error. A small tail of higher error sequences.


```{r, message=FALSE}
pdiffs <- diffs.keep
pdiffs$Qual[is.na(pdiffs$Qual)] <- 1 # Fixed after remapping Qual scores to categories
pdiffs$Quality <- cut(pdiffs$Qual, c(0, 11, 21, 31, 41))
qual.map <- c("(0-11]"="0-11", "(11,21]"="11-21", "(21,31]"="21-31", "(31,41]"="31-41")
pdiffs$Quality <- qual.map[pdiffs$Quality]
pdiffs$Quality[pdiffs$Type=="D"] <- "NA"
pdiffs$Quality <- factor(pdiffs$Quality, levels=c(qual.map, "NA"))
type.map <- c("S"="Substitution", "D"="Deletion", "I"="Insertion")
pdiffs$Error <- factor(type.map[pdiffs$Type], levels=c("Substitution", "Insertion", "Deletion"))
color.scale <- c("hotpink", colorRampPalette(c("deeppink2", "dodgerblue2"))(3))
color.scale <- c(colorRampPalette(c("black", "cyan"))(4), "grey")
# Convert counts to rates by using the total lengths of all kept reads
pdiffs$Rate <- pdiffs$Abund/nnt.keep[pdiffs$QueryPos]
# Force desired facet ymax limits with a custom data.frame, and set desired breaks
dflim <- data.frame(Rate=c(0.001, 0.01, 0.003), 
                    QueryPos=c(200, 200, 200), 
                    Quality=c("NA", "NA", "NA"),
                    Error=c("Substitution", "Insertion", "Deletion"))
my_breaks <- function(x) { 
  if (max(x) < 0.0015) { c("0.0000"=0, "0.0010"=0.001) }  # "0.0005"=0.0005, 
  else if(max(x) < 0.005) { c("0.0000"=0, "0.0010"=0.001, "0.0020"=0.002, "0.0030"=0.003) }
  #  else { c("0.0000"=0, "0.0050"=0.005, "0.0100"=0.01) }
  else { c("0.0000"=0, "0.0020"=0.002, "0.0040"=0.004, "0.0060"=0.006, "0.0080"=0.008, "0.0100"=0.01) }
}
p.err.pos <- ggplot(data=pdiffs, aes(x=QueryPos,y=Rate,color=NULL, fill=Quality)) + geom_col() +
  facet_grid(Error~., scales="free_y") + guides(color=FALSE) + xlab("Nucleotide Position") + ylab("Error Rate") +
  scale_color_manual(values=color.scale) + scale_fill_manual(values=color.scale) + xlim(0, 1500)
#+ geom_blank(data=dflim) + scale_y_continuous(breaks=my_breaks) + theme(axis.text.y=element_text(size=7))
p.err.pos
ggsave(file.path(path.fig, "ErrorRates_Loop.pdf"), p.err.pos, width=12, height=5, units="in", useDingbats=FALSE)
ggsave(file.path(path.fig, "ErrorRates_Loop_1_200.pdf"), p.err.pos + xlim(1,200), width=12, height=5, units="in", useDingbats=FALSE)
```

Calculate total per-base error rates of each type (substitutions, insertions, deletions) over all the reads and positions:
```{r}
tapply(diffs.keep$Abund, diffs.keep$Type, sum)/sum(nnt.keep)
tot.err.rate <- sum(diffs.keep$Abund)/sum(nnt.keep)
tot.err.rate
1. - tot.err.rate
```

# Error modes and optimizing maxEE threshold

Now we're going to take a deeper dive into potential "structural" LoopSeq error modes, inspired by the detection of (a very low number) of chimeras in the filtered LoopSeq data, which supposedly should be entirely absent due to the way initial molecules are barcoded. The approach will be to use much more sensitive settings for DADA2 denoising in order to idenetify errors that stick out the most from the true sequences, and then manually inspect those sequences to generate new hypotheses about LoopSeq structural error modes.

Denoise the filtered data into ASVs, using high-sensitivity DADA2 with `DETECT_SINGLETONS` and `OMEGA_A=1e-10`:
```{r}
dd10 <- dada(filt, err, DETECT_SINGLETONS=TRUE, OMEGA_A = 1e-10, multi=TRUE)
dd10
```

```{r}
is.correct10 <- dd10$sequence %in% dd$sequence
is.correct10
```

The first 27 sequences are the expected ones from the mock community, the next (28-45) are artefacts that are either very different from the true sequences, or are at anomalously high abundance for a random erorr. Using the `$clustering` stats to dig in further:
```{r}
clust.correct <- dd10$clustering[is.correct10,-1]
clust.incorrect <- dd10$clustering[!is.correct10,-1]
clust.correct
clust.incorrect
```

These artefact sequences are all at least `min(clust.incorrect$birth_ham)` nts, and as much as `max(clust.incorrect$birth_ham)` in hamming distance away from the true ASV from which they were split. Additionally, the abundances of the artefact sequences only spans the range from `min(clust.incorrect$abundance)` to `max(clust.incorrect$abundance)` reads. Thus, these look like "structural" errors, in which large numbers of errors are introduced through processes such as chimerization.

As a first pass, let's see if chimeras are identified within these sequences using the standard dada2 bimera-matching approach. Considering :
```{r}
sq10 <- getSequences(dd10); names(sq10) <- paste0("Seq", seq_along(sq10))
sq10.ref <- sq10[sq10 %in% getSequences(dd)]
sq10.incorrect <- sq10[!sq10 %in% getSequences(dd)]
bim10.incorrect <- cbind(isBimeraDenovo(dd10, minFoldParentOverAbundance=4.5), isBimeraDenovo(dd10, minFoldParentOverAbundance=4.5, allowOneOff=TRUE))[sq10.incorrect,]
rownames(bim10.incorrect) <- names(sq10.incorrect)
colnames(bim10.incorrect) <- c("Exact", "Allow One Off")
bim10.incorrect
```

The additional sequences contain at least 3 chimeras identified by the standard left-right bimerea detection method, and another 3 (for a total of 6), when allowing a bit of fuzziness in the form of `allowOneOff=TRUE`. However, what are the rest of these putative structural errors? Now for a manual inspection approach using BLAST against nt and inspection of patterns in the alignments:

```{r}
dada2:::pfasta(sq10.incorrect, id=names(sq10.incorrect))
```

From BLAST against nt, the results look like the following, where "bimera" is indicated by the alignment of the best BLAST hit showing almost all errors occurring at the start or the end of the query sequence, and "introgression" is indicated by almost all errors occurring in a relatively short internal stretch of the query sequence.

Sq28: Bimera
Sq29: Bimera
Sq30: Bimera-ish
Sq31: A short-ish introgression (~50nts?)
Sq32: A short-ish introgression (~50nts?)
Sq33:  short-ish introgression (~25nts?)
Sq34: A short-ish introgression (~30nts?)
Sq35: Introgression (~50nts), plus a couple of mismatches at the beginning?
Sq36: Maybe short introgression? Pattern is less clear
Sq37: Bimera
Sq38: Bimera
Sq39: Bimera
Sq40: Short introgression (~25nts)
Sq41: Not sure, could be a small number of substitutions
Sq42: Likely bimera, small number of differences near start of sequence
Sq43: Introgression (~60nts)
Sq44: Bimera
Sq45: Long-ish Introgression? (~150nts)

Overall there are two take-aways here. First, there are some bimera that are produced in LoopSeq data, that are mostly caught by `isBimeraDenovo(..., allowOneOff=TRUE)` and consistently supported by manual inspection of the BLAST best hit alignments. Second, there seems to be another structural error mode in which short (<200 nts, often <50nts) stretches of DNA possibly from another molecule are introgressed in the middle of another DNA sequence.

Now let's use the known correct seuqences in this dataset to identify evidence for introgressions in a reasonably systematic way. We'll use a moving window approach to scan each incorrect sequence for the best match to a correct sequence from one of the 8 strains, and record which strain it best matches as the window moves along the entire sequence:
```{r}
refs <- dd$sequence[1:27]
names(refs) <- tax[1:27,6]
get.ham <- function(sq, ref, window=50, step=10) {
  # Return, a vector with cols = len(sq)/step 
  # Values are the hammings between sq and the ref in each window
  # with coordinates based on the sq position
  al <- nwalign(sq, ref, band=64, vec=TRUE)
  str1 <- strsplit(al[[1]], "")[[1]] # str1=sq
  str2 <- strsplit(al[[2]], "")[[1]]
  mismatches <- (str1!=str2)
  mismatches <- mismatches[str1 != "-"]
  sapply(seq(1, nchar(sq)-window, step), function(i) sum(mismatches[i:(i+window-1)]))
}
assign.ref <- function(sqi, refs, window=50, step=10) {
  # Find best match of sqi to the provided refs
  hams <- sapply(refs, function(ref) get.ham(sqi, ref=ref, window=window, step=step))
  sapply(seq(nrow(hams)), function(i) {
    matches <- hams[i,] == min(hams[i,])
    matches <- colnames(hams)[matches]
    if(length(unique(matches))==1) {
      unique(matches)
    } else{ NA }
  })
}
```

Check that expected results are obtained on the reference sequences themselves
```{r}
WINDOW=40
assignments.ref <- lapply(sq10.ref, assign.ref, refs=refs, window=WINDOW)
nwindows.ref <- lapply(assignments.ref, length)
df.ref <- data.frame(Assignment=do.call(c, assignments.ref),
                     Sequence=rep(names(sq10.ref), times=nwindows.ref),
                     Position=do.call(c, lapply(nwindows.ref, function(nw) seq(1,nw*10,10))))
ggplot(data=df.ref, aes(x=Position, y=Assignment, color=Assignment)) + 
  geom_point() + ylab(NULL) + facet_wrap(~Sequence) + theme(axis.text.y = element_blank())
```

Everything is consistently assigned, w/ NAs mixed in where the sequences of different taxa can't be differentiated over the window

Now look at the incorrect sequences that were inspected manually above:
```{r}
WINDOW <- 40
assignments <- lapply(sq10.incorrect, assign.ref, refs=refs, window=WINDOW)
nwindows <- lapply(assignments, length)
#sqnames <- paste0("Sq", sapply(seq_along(sq), function(i) sprintf("%02i",i)))
df <- data.frame(Assignment=do.call(c, assignments),
                 Sequence=rep(names(sq10.incorrect), times=nwindows),
                 Position=do.call(c, lapply(nwindows, function(nw) seq(1,nw*10,10))))
ggplot(data=df, aes(x=Position, y=Assignment, color=Assignment)) + 
  geom_point() + ylab(NULL) + facet_wrap(~Sequence) + theme(axis.text.y = element_blank())
```

These results coincide quite well with the manual BLAST inspections, and suggest it would be possible to systematically scan for chimera/introgression errors. 

Automatially classify all reads into categorized error types Correct, SNP/indel, Bimera, Introgression, Contaminant, and Uncategorized with the following definitions:

* Correct: no mismatches to the reference sequences (small length variation is OK)
* Bimera: Identified as bimera by `isBimeraDenovo` or reference scanning method identifies two segments
* Introgression: Reference scanning method identifies three segments, with the middle segment a different taxon
* Contaminant: From a taxon outside the expected community. There aren't any of these here (see above).
* Point Error(s): Not in any of the above catgories, but within 3 hamming distance of a reference sequence (after N-W alignment)
* Uncategorized: The remaining sequences.

Define the function that characterizes bimeras and introgressions from the assignment data:
```{r}
classify.chimera <- function(asn) {
  asn <- asn[!is.na(asn)]
  nunq <- length(unique(asn))
  if(nunq == 1) { return("NonChimera") }
  else if(nunq > 2) { return("Complex") } # Only doing simple chimeras
  else {
    ntransitions <- sum(asn[2:length(asn)] != asn[1:(length(asn)-1)])
    if(ntransitions==1) { return("Chimera") } 
    else if(ntransitions==2) { return("Introgression") }
    else { return("Complex") }
  }
}
```

Test the function on the reference and incorrect sequences manually inspected and visually inspected above:

```{r}
sapply(assignments.ref, classify.chimera)
```

Reference seequences correctly all identified as non-chimeras. Now the incorrect sequences:

```{r}
sapply(assignments, classify.chimera)
```

Again, this matches exactly with the visual plotted results, which matched well with the manual inspection. Function is working as expected.

We'll now perform characterization of all the the sequences prior to filtering and trimming, so we can get the most complete picture of how error rates and types change as a function of quality score thresholds.

Do the base computations:
```{r}
sq.nop <- getSequences(drp.nop)
assignments.nop <- lapply(sq.nop, assign.ref, refs=refs, window=WINDOW)
refclass.nop <- sapply(assignments.nop, classify.chimera)
isbim.nop <- isBimeraDenovo(drp.nop, minFoldParentOverAbundance=4.5, multi=TRUE)
hams.nop <- unname(sapply(sq.nop, function(query) min(nwhamming(query, refs, band=64, vec=TRUE))))
```

Classify the dereplicated unique sequences into the categories described previously:
```{r}
classifications.nop <- rep("Uncategorized", length(sq.nop))
is.correct.nop <- sapply(sq.nop, function(pat, x=refs) any(grepl(pat, x))) # Allows for length differences
is.bimera.nop <- (isbim.nop | refclass.nop == "Chimera") & !is.correct.nop
is.introgression.nop <- refclass.nop == "Introgression" & !is.correct.nop & !is.bimera.nop
is.error.nop <- hams.nop <= 3 & !is.correct.nop & !is.bimera.nop & !is.introgression.nop
classifications.nop[is.correct.nop] <- "Correct"
classifications.nop[is.bimera.nop] <- "Chimera"
classifications.nop[is.introgression.nop] <- "Introgression"
classifications.nop[is.error.nop] <- "Point Error(s)"
table(classifications.nop) # Unique sequences
tapply(drp.nop$uniques, classifications.nop, sum) # Read-weighted
```

Looks good. Now let's plot the rates of errors by type as a function of the expected error threshold.

First read in the raw reads with their associated quality information, and classify them via the dereplicated sequences already classified above.
```{r}
get.readdf <- function(fni) {
  require(ShortRead)
  srq <- readFastq(fni)
  sq <- as.character(sread(srq))
  qq <- as(quality(srq), "matrix")
  mnq <- apply(qq, 1, mean, na.rm=TRUE)
  ee <- dada2:::C_matrixEE(qq)
  data.frame(Sequence=sq, Length=nchar(sq),
             MeanQ = mnq, ExpErr=ee, stringsAsFactors = FALSE)
}
dfr.nop <- get.readdf(nop)
names(classifications.nop) <- sq.nop
dfr.nop$Classification <- classifications.nop[dfr.nop$Sequence]
dfr.nop <- dfr.nop[order(dfr.nop$ExpErr, dfr.nop$Classification=="Correct"),]
```

Now let's plot the fraction of each type as a function of the expected errors threshold.
```{r}
types <- c("Correct", "Chimera", "Introgression", "Point Error(s)", "Uncategorized")
for(type in types) { dfr.nop[[type]] <- cumsum(dfr.nop$Classification == type) }
mdfr.nop <- melt(dfr.nop, measure.vars = types, variable.name="Type", value.name="Reads")
ggplot(data=mdfr.nop, aes(x=ExpErr, y=Reads, color=Type)) + geom_point() + xlim(0,10)
ggplot(data=mdfr.nop, aes(x=ExpErr, y=Reads, color=Type)) + geom_point() + 
  xlim(0,4) + ylim(0, 1000)
```

Very nice, would be even better if this was plotted by fraction of total reads though. Then could also leave out the correct line, which will make ylim automatic sizing work appropriately.

```{r}
for(type in types) { dfr.nop[[type]] <- cumsum(dfr.nop$Classification == type)/seq(nrow(dfr.nop)) }
mdfr.nop <- melt(dfr.nop, measure.vars = types, variable.name="Type", value.name="Rate")
mdfr.nop <- mdfr.nop[order(mdfr.nop$ExpErr, mdfr.nop$Type=="Correct"),]
pmdfr.nop <- mdfr.nop[(nrow(mdfr.nop)/10):nrow(mdfr.nop),] 
# Drop first tenth of data.frame to let cumulative rate estimates stabilize before plotting them
ggplot(data=pmdfr.nop, aes(x=ExpErr, y=Rate, color=Type)) + 
  geom_point() + xlim(0,10) + xlab("Maximum Expected Error") + ylab("Fraction Reads with Error") +
  labs(color = "Error Type")
ggplot(data=pmdfr.nop[!pmdfr.nop$Type == "Correct",], aes(x=ExpErr, y=Rate, color=Type)) + 
  geom_point() + xlim(0,4) + xlab("Maximum Expected Error") + ylab("Fraction Reads with Error") +
  labs(color = "Error Type")
```

Looks pretty good. Should include the fraction of reads kept at these EE thresholds as well. Clear evidence that a threshold of ~0.5 would be very effective. Maybe even lower, although then balancing against loss of reads starts to matter more.

Plot fraction of reads kept vs. error rates? How about per-base error rates as a function of expected error threshold?

```{r}
dfr.nop$Retained <- seq(nrow(dfr.nop))/nrow(dfr.nop)
dfr.nop$Removed <- 1-dfr.nop$Retained
ggplot(data=dfr.nop, aes(x=ExpErr, y=Removed)) + geom_line() + xlim(0,4)
```

```{r}
pbar <- ggplot(data=pmdfr.nop[!pmdfr.nop$Type == "Correct",], aes(x=ExpErr, y=Rate, color=Type))
SCALE <- 0.05
pbar <- pbar + geom_line(data=dfr.nop, aes(x=ExpErr, y=Removed*SCALE), color="red")
pbar <- pbar + scale_y_continuous(sec.axis = sec_axis(~.*1/SCALE, name="Fraction Reads Removed by Filter"))
pbar <- pbar + theme(axis.title.y.right = element_text(color="red"), axis.text.y.right=element_text(color="red"))
# From http://colorbrewer2.org/#type=qualitative&scheme=Paired&n=4
color.scale <- c("Chimera"="#a6cee3", "Introgression"="#1f78b4", 
                 "Point Error(s)"="#b2df8a", "Uncategorized"="#33a02c")
pbar <- pbar + geom_point() + 
  xlim(0,4) + xlab("Maximum Expected Errors Threshold") + ylab("Fraction Reads with Error") +
  scale_color_manual(values=color.scale) +
  labs(color = "Error Type")
pbar
ggsave(file.path(path.fig, "ErrorsByTypeAndMaxEE.pdf"), pbar,
                 width=5.5, height=3, units="in", useDingbats=FALSE)
ggsave(file.path(path.fig, "ErrorsByTypeAndMaxEE.png"), pbar,
                 width=5.5, height=3, units="in")
```

Officially recommend a `maxEE=1` or less filter for Loop data. At a loss of ~10% of reads, reduces structural errors to <1% of total reads, and total reads with errors to ~4% of total reads.

Implement this recommendation, and evaluate high-sensitivity DADA2:
```{r}
filt.opt <- file.path(path, "filtered", "optimized", basename(fn))
track.opt <- filterAndTrim(nop, filt.opt, maxEE=0.5, minLen=1400, maxLen=1600, verbose=TRUE)
```

```{r}
err.opt <- learnErrors(filt.opt, multi=TRUE)
dd.opt <- dada(filt.opt, err=err.opt, multi=TRUE, DETECT_SINGLETONS=TRUE, OMEGA_A=1e-10)
unq.opt <- removeBimeraDenovo(dd.opt, minFoldParentOverAbundance=4.5, verbose=TRUE)
table(getSequences(unq.opt) %in% refs)
dd.opt$clustering[!dd.opt$sequence %in% refs, -1]
```

It's not perfect, but it's not far off! Just 4 singleton ASVs sneak through with those high sensitivity settings used.

```{r}
sessionInfo()
```